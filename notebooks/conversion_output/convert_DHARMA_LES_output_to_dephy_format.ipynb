{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Example: convert DHARMA LES output to DEPHY format\n",
    "\n",
    "Code to read DHARMA LES output files and write to DEPHY format (NetCDF)\n",
    "\n",
    "Contributed by Ann Fridlind from NASA/GISS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import os\n",
    "import netCDF4\n",
    "import datetime as dt\n",
    "from netCDF4 import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify directory locations\n",
    "\n",
    "If on the ARM JupyterHub, it is recommended to create and specify a local directory that is outside of the COMBLE-MIP repository to upload raw model output files in your model's format. \n",
    "\n",
    "Processed domain-mean outputs are invited for commit to the GitHub repository on a user-specified branch under /comble-mip/output_les/YOUR_MODEL_NAME/sandbox/YOUR_RUN_NAME. These can be committed and removed at any time.\n",
    "\n",
    "It is requested to name your baseline small-domain run directory as 'Lx25km_dx100m' (in place of YOUR_RUN_NAME), so that it can readily be automatically compared with other runs using the same test specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/data/home/floriantornow/dharma/sandbox/case0313_diag_ice25_nomiz_dx200/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 30\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#my_readdir = 'case0313_diag_ice25_nomiz_dx200_specZ0'\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#my_outfile = 'DHARMA_Lx25_dx200_FixN.nc'\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m \n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# specify local source directories (additional subdirectories if restart was required)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m my_rundir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/data/home/floriantornow/dharma/sandbox/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m my_readdir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 30\u001b[0m my_outdirs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m([f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_rundir\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)], key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m\u001b[38;5;241m.\u001b[39mlower)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(my_outdirs)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# specify Github scratch directory where processed model output will be committed\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/data/home/floriantornow/dharma/sandbox/case0313_diag_ice25_nomiz_dx200/'"
     ]
    }
   ],
   "source": [
    "# specify start time of simulation and simulation name\n",
    "start_dtime = '2020-03-12 22:00:00.0'\n",
    "\n",
    "# specify input data directory name, traceable to source machine, and\n",
    "# specify output file name (see file naming convention in TOC)\n",
    "\n",
    "# FixN with no ice test\n",
    "my_readdir = 'case0313_diag_ice0_nomiz_dx100_specZ0'\n",
    "my_outfile = 'DHARMA_Lx25_dx100_FixN_noice.nc'\n",
    "\n",
    "# FixN with ice test\n",
    "my_readdir = 'case0313_diag_ice25_nomiz_dx100_specZ0'\n",
    "my_outfile = 'DHARMA_Lx25_dx100_FixN.nc'\n",
    "\n",
    "# FixN with ice test and default z0\n",
    "my_readdir = 'case0313_diag_ice25_nomiz_dx200' ## Please update with your run, Ann.\n",
    "my_outfile = 'DHARMA_Lx25_dx200_FixN_def_z0.nc'\n",
    "\n",
    "#my_readdir = 'case0313_diag_ice25_nomiz_dx200_specZ0'\n",
    "#my_outfile = 'DHARMA_Lx25_dx200_FixN.nc'\n",
    "\n",
    "# ProgNa with ice test\n",
    "#my_readdir = 'case0313_prog_ice25_nomiz_dx100_specZ0'\n",
    "#my_outfile = 'DHARMA_Lx25_dx100_ProgNa.nc'\n",
    "\n",
    "# specify local source directories (additional subdirectories if restart was required)\n",
    "\n",
    "my_rundir = '/data/home/floriantornow/dharma/sandbox/' + my_readdir + '/'\n",
    "\n",
    "my_outdirs = sorted([f for f in os.listdir(my_rundir) if not f.startswith('.')], key=str.lower)\n",
    "print(my_outdirs)\n",
    "\n",
    "# specify Github scratch directory where processed model output will be committed\n",
    "my_gitdir = '../../output_les/dharma/sandbox/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read DHARMA data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read set-up parameters\n",
    "\n",
    "Note: ERROR 1: PROJ... message can be ignored here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/data/home/floriantornow/dharma/sandbox/case0313_diag_ice25_nomiz_dx200/0-20h/dharma.cdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xarray/backends/file_manager.py:211\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 211\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_key\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xarray/backends/lru_cache.py:56\u001b[0m, in \u001b[0;36mLRUCache.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m---> 56\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39mmove_to_end(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: [<class 'netCDF4._netCDF4.Dataset'>, ('/data/home/floriantornow/dharma/sandbox/case0313_diag_ice25_nomiz_dx200/0-20h/dharma.cdf',), 'r', (('clobber', True), ('diskless', False), ('format', 'NETCDF4'), ('persist', False)), '6c8fb123-5e16-4310-92d9-4638381d8ca9']",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# read in DHARMA parameter settings\u001b[39;00m\n\u001b[1;32m      2\u001b[0m input_filename \u001b[38;5;241m=\u001b[39m my_rundir \u001b[38;5;241m+\u001b[39m my_outdirs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/dharma.cdf\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m dharma_params \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_filename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(input_filename)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# check if the run contains ice variables\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xarray/backends/api.py:573\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m decoders \u001b[38;5;241m=\u001b[39m _resolve_decoders_kwargs(\n\u001b[1;32m    562\u001b[0m     decode_cf,\n\u001b[1;32m    563\u001b[0m     open_backend_dataset_parameters\u001b[38;5;241m=\u001b[39mbackend\u001b[38;5;241m.\u001b[39mopen_dataset_parameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    569\u001b[0m     decode_coords\u001b[38;5;241m=\u001b[39mdecode_coords,\n\u001b[1;32m    570\u001b[0m )\n\u001b[1;32m    572\u001b[0m overwrite_encoded_chunks \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite_encoded_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 573\u001b[0m backend_ds \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdecoders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m ds \u001b[38;5;241m=\u001b[39m _dataset_from_backend_dataset(\n\u001b[1;32m    580\u001b[0m     backend_ds,\n\u001b[1;32m    581\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    592\u001b[0m )\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xarray/backends/netCDF4_.py:646\u001b[0m, in \u001b[0;36mNetCDF4BackendEntrypoint.open_dataset\u001b[0;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, format, clobber, diskless, persist, lock, autoclose)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen_dataset\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]  # allow LSP violation, not supporting **kwargs\u001b[39;00m\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    627\u001b[0m     filename_or_obj: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m os\u001b[38;5;241m.\u001b[39mPathLike[Any] \u001b[38;5;241m|\u001b[39m BufferedIOBase \u001b[38;5;241m|\u001b[39m AbstractDataStore,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    643\u001b[0m     autoclose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    644\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[1;32m    645\u001b[0m     filename_or_obj \u001b[38;5;241m=\u001b[39m _normalize_path(filename_or_obj)\n\u001b[0;32m--> 646\u001b[0m     store \u001b[38;5;241m=\u001b[39m \u001b[43mNetCDF4DataStore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclobber\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclobber\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdiskless\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiskless\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpersist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoclose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    658\u001b[0m     store_entrypoint \u001b[38;5;241m=\u001b[39m StoreBackendEntrypoint()\n\u001b[1;32m    659\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m close_on_error(store):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xarray/backends/netCDF4_.py:409\u001b[0m, in \u001b[0;36mNetCDF4DataStore.open\u001b[0;34m(cls, filename, mode, format, group, clobber, diskless, persist, lock, lock_maker, autoclose)\u001b[0m\n\u001b[1;32m    403\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    404\u001b[0m     clobber\u001b[38;5;241m=\u001b[39mclobber, diskless\u001b[38;5;241m=\u001b[39mdiskless, persist\u001b[38;5;241m=\u001b[39mpersist, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m\n\u001b[1;32m    405\u001b[0m )\n\u001b[1;32m    406\u001b[0m manager \u001b[38;5;241m=\u001b[39m CachingFileManager(\n\u001b[1;32m    407\u001b[0m     netCDF4\u001b[38;5;241m.\u001b[39mDataset, filename, mode\u001b[38;5;241m=\u001b[39mmode, kwargs\u001b[38;5;241m=\u001b[39mkwargs\n\u001b[1;32m    408\u001b[0m )\n\u001b[0;32m--> 409\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoclose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xarray/backends/netCDF4_.py:356\u001b[0m, in \u001b[0;36mNetCDF4DataStore.__init__\u001b[0;34m(self, manager, group, mode, lock, autoclose)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group \u001b[38;5;241m=\u001b[39m group\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m mode\n\u001b[0;32m--> 356\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mds\u001b[49m\u001b[38;5;241m.\u001b[39mdata_model\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds\u001b[38;5;241m.\u001b[39mfilepath()\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_remote \u001b[38;5;241m=\u001b[39m is_remote_uri(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xarray/backends/netCDF4_.py:418\u001b[0m, in \u001b[0;36mNetCDF4DataStore.ds\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mds\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_acquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xarray/backends/netCDF4_.py:412\u001b[0m, in \u001b[0;36mNetCDF4DataStore._acquire\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_acquire\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 412\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manager\u001b[38;5;241m.\u001b[39macquire_context(needs_lock) \u001b[38;5;28;01mas\u001b[39;00m root:\n\u001b[1;32m    413\u001b[0m         ds \u001b[38;5;241m=\u001b[39m _nc4_require_group(root, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode)\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/contextlib.py:135\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xarray/backends/file_manager.py:199\u001b[0m, in \u001b[0;36mCachingFileManager.acquire_context\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;129m@contextlib\u001b[39m\u001b[38;5;241m.\u001b[39mcontextmanager\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21macquire_context\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    198\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Context manager for acquiring a file.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m     file, cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_acquire_with_cache_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneeds_lock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m file\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xarray/backends/file_manager.py:217\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    215\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    216\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode\n\u001b[0;32m--> 217\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_opener\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# ensure file doesn't get overridden when opened again\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2449\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4.Dataset.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2012\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/data/home/floriantornow/dharma/sandbox/case0313_diag_ice25_nomiz_dx200/0-20h/dharma.cdf'"
     ]
    }
   ],
   "source": [
    "# read in DHARMA parameter settings\n",
    "input_filename = my_rundir + my_outdirs[0] + '/dharma.cdf'\n",
    "dharma_params = xr.open_dataset(input_filename)\n",
    "print(input_filename)\n",
    "\n",
    "# check if the run contains ice variables\n",
    "do_ice = bool(dharma_params['Cond'].do_ice)\n",
    "print('do_ice = ',do_ice)\n",
    "\n",
    "# check for prognostic aerosol\n",
    "do_progNa = bool(dharma_params['Cond'].do_prog_na)\n",
    "print('do_progNa = ',do_progNa)\n",
    "\n",
    "# full parameter list\n",
    "dharma_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read domain-mean profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# concatenate DHARMA domain-mean instantaneous profiles and take 10-min average:\n",
    "# resample-average before concatenating and removing duplicates\n",
    "for index, elem in enumerate(my_outdirs):\n",
    "    input_filename = my_rundir + elem + '/dharma.soundings.cdf'\n",
    "    print(input_filename)\n",
    "    if index==0:\n",
    "        dharma_snds = xr.open_dataset(input_filename)\n",
    "        dharma_snds['time'] = pd.to_datetime(dharma_snds['time'].values, unit='s', origin=pd.Timestamp(start_dtime))\n",
    "        dharma_snds = dharma_snds.resample(time=\"600s\",closed=\"right\",label=\"right\").mean()\n",
    "    else:\n",
    "        dharma_snds2 = xr.open_dataset(input_filename)\n",
    "        dharma_snds2['time'] = pd.to_datetime(dharma_snds2['time'].values, unit='s', origin=pd.Timestamp(start_dtime))\n",
    "        dharma_snds2 = dharma_snds2.resample(time=\"600s\",closed=\"right\",label=\"right\").mean()\n",
    "        dharma_snds = xr.concat([dharma_snds,dharma_snds2],dim='time')\n",
    "dharma_snds = dharma_snds.drop_duplicates('time',keep='first')\n",
    "dharma_snds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profile unit conversions and sundry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a dummy sounding and initialize some variables needed\n",
    "dummy_snd = dharma_snds['qc']*0.\n",
    "nz = dharma_params['geometry'].nz\n",
    "dz = dharma_snds['zw'].data[1:nz+1]-dharma_snds['zw'].data[0:nz]\n",
    "cp = 1004.\n",
    "Lhe = 2.50e6\n",
    "Lhs = Lhe + 3.34e5\n",
    "\n",
    "# compute some intermediate quantities for use below\n",
    "Fql_turb = dharma_snds['Fqc_turb'].data+dharma_snds['Fqr_turb'].data\n",
    "if do_ice:\n",
    "    Fqi_turb = dharma_snds['Fqic_turb'].data+dharma_snds['Fqif_turb'].data+dharma_snds['Fqid_turb'].data\n",
    "wql_tot = 0.5*(Fql_turb[:,0:nz]+Fql_turb[:,1:nz+1])/dharma_snds['rhobar'].data+dharma_snds['WQL'].data\n",
    "\n",
    "if do_ice:\n",
    "    wqi_tot = 0.5*(Fqi_turb[:,0:nz]+Fqi_turb[:,1:nz+1])/dharma_snds['rhobar'].data+dharma_snds['WQI'].data\n",
    "else:\n",
    "    wqi_tot = np.nan*wql_tot\n",
    "PFql = dharma_snds['PFqc'].data+dharma_snds['PFqr'].data\n",
    "if do_ice:\n",
    "    PFqi = dharma_snds['PFqic'].data+dharma_snds['PFqif'].data+dharma_snds['PFqid'].data\n",
    "else:\n",
    "    PFqi = np.nan*PFql\n",
    "wpfl = 0.5*(PFql[:,0:nz]+PFql[:,1:nz+1])*-1./3600./dharma_snds['rhobar'].data\n",
    "wpfi = 0.5*(PFqi[:,0:nz]+PFqi[:,1:nz+1])*-1./3600./dharma_snds['rhobar'].data\n",
    "PFqc = dharma_snds['PFqc'].data \n",
    "PFqr = dharma_snds['PFqr'].data \n",
    "if do_ice:\n",
    "    PFqic = dharma_snds['PFqic'].data \n",
    "    PFqif = dharma_snds['PFqif'].data \n",
    "    PFqid = dharma_snds['PFqid'].data \n",
    "if do_progNa:\n",
    "    ssa_sfc = (dharma_snds['Sna_1_sfc'].data[:,0]+dharma_snds['Sna_2_sfc'].data[:,0]+dharma_snds['Sna_3_sfc'].data[:,0])*dharma_snds['zw'].data[1]\n",
    "Flwd = dharma_snds['Flw_dn'].data\n",
    "Flwu = dharma_snds['Flw_up'].data\n",
    "Fnlw = Flwu - Flwd\n",
    "Suvar_adv = dharma_snds['Su2_adv'].data - dharma_snds['Subar2_adv'].data\n",
    "Svvar_adv = dharma_snds['Sv2_adv'].data - dharma_snds['Svbar2_adv'].data\n",
    "Swvar_adv = dharma_snds['Sw2_adv'].data - dharma_snds['Swbar2_adv'].data\n",
    "Stke_a = Suvar_adv + Svvar_adv + 0.5*(Swvar_adv[:,0:nz]+Swvar_adv[:,1:nz+1])\n",
    "Stke_adv_dis = dharma_snds['Stke_adv'].data + dharma_snds['Sprod'].data\n",
    "Smke = (dharma_snds['u'].data-dharma_params.translate.u)*dharma_snds['Suavg_SGS'].data + (dharma_snds['v'].data-dharma_params.translate.v)*dharma_snds['Svavg_SGS'].data\n",
    "Ske = dharma_snds['Su2avg_SGS'].data+dharma_snds['Sv2avg_SGS'].data+dharma_snds['Sw2avg_SGS'].data\n",
    "Stke_dis = Smke - Ske\n",
    "\n",
    "# append new variables to the data structure\n",
    "dharma_snds = dharma_snds.assign(theta = dummy_snd + (dharma_snds['th'].data+1)*dharma_snds.theta_00)\n",
    "dharma_snds = dharma_snds.assign(pi = dummy_snd + dharma_snds['T'].data/dharma_snds['theta'].data)\n",
    "dharma_snds = dharma_snds.assign(pressure = dummy_snd + np.power(dharma_snds['pi'].data,7./2)*np.power(10.,5))\n",
    "dharma_snds = dharma_snds.assign(PF = dummy_snd + 0.5*(PFqc[:,0:nz]+PFqc[:,1:nz+1]) + 0.5*(PFqr[:,0:nz]+PFqr[:,1:nz+1]))\n",
    "dharma_snds = dharma_snds.assign(nlcic = dummy_snd + dharma_snds['nc_cld'].data*1.e6/dharma_snds['rhobar'].data)\n",
    "if do_ice and do_progNa:\n",
    "    dharma_snds = dharma_snds.assign(niic = dummy_snd + dharma_snds['ni_cld'].data*1.e6/dharma_snds['rhobar'].data)\n",
    "if do_ice:\n",
    "    dharma_snds = dharma_snds.assign(PFi = dummy_snd + 0.5*(PFqic[:,0:nz]+PFqic[:,1:nz+1]) + 0.5*(PFqif[:,0:nz]+PFqif[:,1:nz+1]) \n",
    "                                     + 0.5*(PFqid[:,0:nz]+PFqid[:,1:nz+1]) )\n",
    "    dharma_snds['PF'] += dharma_snds['PFi']\n",
    "else:\n",
    "    dharma_snds['RHI'] = np.nan*dharma_snds['RH']\n",
    "    dharma_snds['PFi'] = np.nan*dharma_snds['PF']\n",
    "dharma_snds = dharma_snds.assign(uw_zt = dummy_snd + 0.5*(dharma_snds['txz_tot'].data[:,0:nz]+dharma_snds['txz_tot'].data[:,1:nz+1]))\n",
    "dharma_snds = dharma_snds.assign(vw_zt = dummy_snd + 0.5*(dharma_snds['tyz_tot'].data[:,0:nz]+dharma_snds['tyz_tot'].data[:,1:nz+1]))\n",
    "dharma_snds = dharma_snds.assign(w2_zt = dummy_snd + (dharma_snds['w2'].data[:,0:nz]+dharma_snds['w2'].data[:,1:nz+1])) # w2 = 0.5*w'2\n",
    "dharma_snds = dharma_snds.assign(wth_zt = dummy_snd + 0.5*(dharma_snds['qhz_tot'].data[:,0:nz] + \n",
    "                                        dharma_snds['qhz_tot'].data[:,1:nz+1])*dharma_snds.theta_00)\n",
    "dharma_snds = dharma_snds.assign(wthli_zt = dummy_snd + dharma_snds['wth_zt'].data\n",
    "                    - wql_tot*Lhe/(dharma_snds['pi'].data*cp) - wqi_tot*Lhs/(dharma_snds['pi'].data*cp)\n",
    "                    - wpfl*Lhe/(dharma_snds['pi'].data*cp) - wpfi*Lhs/(dharma_snds['pi'].data*cp))                                 \n",
    "dharma_snds = dharma_snds.assign(wqv_zt = dummy_snd + 0.5*(dharma_snds['qqz_tot'].data[:,0:nz]+dharma_snds['qqz_tot'].data[:,1:nz+1]))\n",
    "dharma_snds = dharma_snds.assign(wqt_zt = dummy_snd + dharma_snds['wqv_zt'].data + wql_tot + wqi_tot + wpfl + wpfi)\n",
    "dharma_snds = dharma_snds.assign(eps = dummy_snd + Stke_dis + Stke_adv_dis)\n",
    "dharma_snds = dharma_snds.assign(LWdn = dummy_snd + 0.5*(Flwd[:,0:nz]+Flwd[:,1:nz+1]))\n",
    "dharma_snds = dharma_snds.assign(LWup = dummy_snd + 0.5*(Flwu[:,0:nz]+Flwu[:,1:nz+1]))\n",
    "dharma_snds = dharma_snds.assign(HRlw = dummy_snd + 0.5*(Fnlw[:,0:nz]+Fnlw[:,1:nz+1])/dz/dharma_snds['rhobar'].data)\n",
    "dharma_snds = dharma_snds.assign(dth_micro = dummy_snd + dharma_snds['Sth_micro'].data + dharma_snds['Sth_cond'].data)\n",
    "dharma_snds = dharma_snds.assign(dq_micro = dummy_snd + dharma_snds['Sqv_micro'].data + dharma_snds['Sqv_cond'].data)\n",
    "#dharma_snds = dharma_snds.assign(dth_turb = dummy_snd + (dharma_snds['qhz_tot'].data[:,0:nz] - \n",
    "#                                        dharma_snds['qhz_tot'].data[:,1:nz+1])*dharma_snds.theta_00)\n",
    "#dharma_snds = dharma_snds.assign(dq_turb = dummy_snd + dharma_snds['qqz_tot'].data[:,0:nz] - dharma_snds['qqz_tot'].data[:,1:nz+1])\n",
    "dharma_snds = dharma_snds.assign(dth_turb = dummy_snd + (dharma_snds['qhz_tot'].data[:,0:nz] - \n",
    "                                                         dharma_snds['qhz_tot'].data[:,1:nz+1])/dz)\n",
    "dharma_snds = dharma_snds.assign(dq_turb = dummy_snd + (dharma_snds['qqz_tot'].data[:,0:nz] - dharma_snds['qqz_tot'].data[:,1:nz+1])/dz)\n",
    "\n",
    " \n",
    "if do_progNa:\n",
    "    dharma_snds = dharma_snds.assign(na_loss_liq = dummy_snd + dharma_snds['na_loss_prof'].data - dharma_snds['na_loss_ice'].data)\n",
    "    dharma_snds = dharma_snds.assign(dna_mixing = dummy_snd + dharma_snds['Sna_1_adv'].data + dharma_snds['Sna_2_adv'].data + dharma_snds['Sna_3_adv'].data + \n",
    "                                dharma_snds['Sna_1_sfc'].data + dharma_snds['Sna_2_sfc'].data + dharma_snds['Sna_3_sfc'].data)\n",
    "\n",
    "dharma_snds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read domain-mean scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for index, elem in enumerate(my_outdirs):\n",
    "    input_filename = my_rundir + elem + '/dharma.scalars.cdf'\n",
    "    print(input_filename)\n",
    "    if index==0:\n",
    "        dharma_scas = xr.open_dataset(input_filename)\n",
    "        dharma_scas['time'] = pd.to_datetime(dharma_scas['time'].values, unit='s', origin=pd.Timestamp(start_dtime))\n",
    "        dharma_scas = dharma_scas.resample(time=\"600s\",closed=\"right\",label=\"right\").mean()\n",
    "    else:\n",
    "        dharma_scas2 = xr.open_dataset(input_filename)\n",
    "        dharma_scas2['time'] = pd.to_datetime(dharma_scas2['time'].values, unit='s', origin=pd.Timestamp(start_dtime))\n",
    "        dharma_scas2 = dharma_scas2.resample(time=\"600s\",closed=\"right\",label=\"right\").mean()\n",
    "        dharma_scas = xr.concat([dharma_scas,dharma_scas2],dim='time')\n",
    "dharma_scas = dharma_scas.drop_duplicates('time',keep='first')\n",
    "dharma_scas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate some additional variables requested\n",
    "dummy_sca = dharma_scas['lwp']*0.\n",
    "dharma_scas = dharma_scas.assign(Psurf = dummy_sca + dharma_params['sounding'].Psurf*100.)\n",
    "if do_ice:\n",
    "    dharma_scas = dharma_scas.assign(opd_tot = dummy_sca + dharma_scas['opd_drops'].data + dharma_scas['opd_ice'].data)\n",
    "else:\n",
    "    dharma_scas = dharma_scas.assign(opd_tot = dummy_sca + dharma_scas['opd_drops'].data)\n",
    "    #dharma_scas = dharma_scas.assign(RHI = np.nan*dharma_scas['opd_tot'])\n",
    "dharma_scas = dharma_scas.assign(LWdnSFC = dummy_sca + dharma_snds['Flw_dn'].data[:,0])\n",
    "dharma_scas = dharma_scas.assign(LWupSFC = dummy_sca + dharma_snds['Flw_up'].data[:,0])\n",
    "dharma_scas = dharma_scas.assign(avg_precip_ice = dummy_sca + dharma_scas['avg_precip'].data \n",
    "                                 - dharma_snds['PFqc'].data[:,0] - dharma_snds['PFqr'].data[:,0])\n",
    "if do_progNa:\n",
    "    dharma_scas = dharma_scas.assign(ssaf = dummy_sca + ssa_sfc)\n",
    "dharma_scas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare output file in DEPHY format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read requested variables list\n",
    "\n",
    "Variable description, naming, units, and dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read list of requested variables\n",
    "vars_mean_list = pd.read_excel('https://docs.google.com/spreadsheets/d/1Vl8jYGviet7EtXZuQiitrx4NSkV1x27aJAhxxjBb9zI/export?gid=0&format=xlsx',\n",
    "                              sheet_name='Mean')\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "vars_mean_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match DHARMA variables to requested outputs\n",
    "\n",
    "Expand the table to include columns that indicate DHARMA model variable names and any conversion factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# drop comments\n",
    "vars_mean_list = vars_mean_list#.drop(columns='comment (10-min average reported at endpoints, green=minimum)')\n",
    "\n",
    "# add columns to contain model output name and units conversion factors\n",
    "vars_mean_list = vars_mean_list.assign(model_name='missing data',conv_factor=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# identify requested variables with only time dimension\n",
    "vars_mean_scas = vars_mean_list[vars_mean_list['dimensions']=='time']\n",
    "\n",
    "# match to DHARMA variable names and specify conversion factors\n",
    "for index in vars_mean_scas.index:\n",
    "    standard_name = vars_mean_list.standard_name.iat[index]\n",
    "    if standard_name=='surface_pressure': \n",
    "        vars_mean_list.model_name.iat[index] = 'Psurf'\n",
    "    if standard_name=='surface_temperature': \n",
    "        vars_mean_list.model_name.iat[index] = 'avg_T_sfc'\n",
    "    if standard_name=='surface_friction_velocity': \n",
    "        vars_mean_list.model_name.iat[index] = 'avg_ustar'\n",
    "    if standard_name=='surface_roughness_length_for_momentum_in_air':\n",
    "        vars_mean_list.model_name.iat[index] = 'avg_z0'\n",
    "    if standard_name=='surface_roughness_length_for_heat_in_air':\n",
    "        vars_mean_list.model_name.iat[index] = 'avg_z0h'\n",
    "    if standard_name=='surface_roughness_length_for_humidity_in_air':\n",
    "        # same as roughness length for heat in DHARMA\n",
    "        vars_mean_list.model_name.iat[index] = 'avg_z0h'\n",
    "    if standard_name=='surface_upward_sensible_heat_flux': \n",
    "        vars_mean_list.model_name.iat[index] = 'avg_T_flx'\n",
    "    if standard_name=='surface_upward_latent_heat_flux': \n",
    "        vars_mean_list.model_name.iat[index] = 'avg_qv_flx'\n",
    "    if standard_name=='obukhov_length': \n",
    "        vars_mean_list.model_name.iat[index] = 'avg_obk'\n",
    "    if standard_name=='atmosphere_mass_content_of_liquid_cloud_water': \n",
    "        vars_mean_list.model_name.iat[index] = 'cwp'\n",
    "        vars_mean_list.conv_factor.iat[index] = 1/1000.\n",
    "    if standard_name=='atmosphere_mass_content_of_rain_water': \n",
    "        vars_mean_list.model_name.iat[index] = 'rwp'\n",
    "        vars_mean_list.conv_factor.iat[index] = 1/1000.\n",
    "    if do_ice:\n",
    "        if standard_name=='atmosphere_mass_content_of_ice_water': \n",
    "            vars_mean_list.model_name.iat[index] = 'iwp'\n",
    "            vars_mean_list.conv_factor.iat[index] = 1/1000.\n",
    "    if standard_name=='cloud_area_fraction': \n",
    "        vars_mean_list.model_name.iat[index] = 'colf_opd'\n",
    "    if standard_name=='optical_depth': \n",
    "        vars_mean_list.model_name.iat[index] = 'opd_tot'\n",
    "    if standard_name=='optical_depth_of_liquid_cloud': \n",
    "        vars_mean_list.model_name.iat[index] = 'opd_cloud'\n",
    "    if standard_name=='precipitation_flux_at_surface': \n",
    "        vars_mean_list.model_name.iat[index] = 'avg_precip'\n",
    "        vars_mean_list.conv_factor.iat[index] = 1/3600.\n",
    "    if do_ice:\n",
    "        if standard_name=='precipitation_flux_at_surface_in_ice_phase': \n",
    "            vars_mean_list.model_name.iat[index] = 'avg_precip_ice'\n",
    "            vars_mean_list.conv_factor.iat[index] = 1/3600.\n",
    "    if standard_name=='optical_depth_of_cloud_droplets': \n",
    "        vars_mean_list.model_name.iat[index] = 'opd_cloud'\n",
    "    if standard_name=='toa_outgoing_longwave_flux': \n",
    "        vars_mean_list.model_name.iat[index] = 'LWupTOA'\n",
    "    if standard_name=='surface_downwelling_longwave_flux': \n",
    "        vars_mean_list.model_name.iat[index] = 'LWdnSFC'  \n",
    "    if standard_name=='surface_upwelling_longwave_flux': \n",
    "        vars_mean_list.model_name.iat[index] = 'LWupSFC'\n",
    "    if do_progNa:\n",
    "        if standard_name=='surface_sea_spray_number_flux': \n",
    "            vars_mean_list.model_name.iat[index] = 'ssaf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# identify requested variables with time and vertical dimensions\n",
    "vars_mean_snds = vars_mean_list[vars_mean_list['dimensions']=='time, height']\n",
    "\n",
    "# match to DHARMA variable names and specify conversion factors\n",
    "for index in vars_mean_snds.index:\n",
    "    standard_name = vars_mean_list.standard_name.iat[index]\n",
    "    if standard_name=='air_pressure': \n",
    "        vars_mean_list.model_name.iat[index] = 'pressure'\n",
    "    if standard_name=='eastward_wind': \n",
    "        vars_mean_list.model_name.iat[index] = 'u'\n",
    "    if standard_name=='northward_wind': \n",
    "        vars_mean_list.model_name.iat[index] = 'v'\n",
    "    if standard_name=='air_dry_density': \n",
    "        vars_mean_list.model_name.iat[index] = 'rhobar'\n",
    "    if standard_name=='air_temperature': \n",
    "        vars_mean_list.model_name.iat[index] = 'T'\n",
    "    if standard_name=='water_vapor_mixing_ratio': \n",
    "        vars_mean_list.model_name.iat[index] = 'qv'\n",
    "    if standard_name=='relative_humidity': \n",
    "        vars_mean_list.model_name.iat[index] = 'RH'\n",
    "        vars_mean_list.conv_factor.iat[index] = 1/100.\n",
    "    if standard_name=='relative_humidity_over_ice': \n",
    "        vars_mean_list.model_name.iat[index] = 'RHI'\n",
    "        vars_mean_list.conv_factor.iat[index] = 1/100.\n",
    "    if standard_name=='air_potential_temperature': \n",
    "        vars_mean_list.model_name.iat[index] = 'theta'\n",
    "    if standard_name=='specific_turbulent_kinetic_energy_resolved': \n",
    "        vars_mean_list.model_name.iat[index] = 'tkeavg'\n",
    "    if standard_name=='specific_turbulent_kinetic_energy_sgs': \n",
    "        vars_mean_list.model_name.iat[index] = 'tke_smag'\n",
    "    if standard_name=='mass_mixing_ratio_of_cloud_liquid_water_in_air': \n",
    "        vars_mean_list.model_name.iat[index] = 'qc'\n",
    "    if standard_name=='mass_mixing_ratio_of_rain_water_in_air': \n",
    "        vars_mean_list.model_name.iat[index] = 'qr'\n",
    "    if do_ice:\n",
    "        if standard_name=='mass_mixing_ratio_of_cloud_ice_in_air': \n",
    "            vars_mean_list.model_name.iat[index] = 'qic'\n",
    "        if standard_name=='mass_mixing_ratio_of_snow_in_air': \n",
    "            vars_mean_list.model_name.iat[index] = 'qif'\n",
    "        if standard_name=='mass_mixing_ratio_of_graupel_in_air': \n",
    "            vars_mean_list.model_name.iat[index] = 'qid'\n",
    "    if standard_name=='number_of_liquid_cloud_droplets_in_air': \n",
    "        vars_mean_list.model_name.iat[index] = 'nc'\n",
    "    if standard_name=='number_of_rain_drops_in_air': \n",
    "        vars_mean_list.model_name.iat[index] = 'nr'\n",
    "    if do_ice:\n",
    "        if standard_name=='number_of_cloud_ice_crystals_in_air': \n",
    "            vars_mean_list.model_name.iat[index] = 'nic'\n",
    "        if standard_name=='number_of_snow_crystals_in_air': \n",
    "            vars_mean_list.model_name.iat[index] = 'nif'\n",
    "        if standard_name=='number_of_graupel_crystals_in_air': \n",
    "            vars_mean_list.model_name.iat[index] = 'nid'    \n",
    "    if do_progNa:\n",
    "        if standard_name=='number_of_total_aerosol_mode1': \n",
    "            vars_mean_list.model_name.iat[index] = 'na_1'\n",
    "        if standard_name=='number_of_total_aerosol_mode2': \n",
    "            vars_mean_list.model_name.iat[index] = 'na_2'\n",
    "        if standard_name=='number_of_total_aerosol_mode3': \n",
    "            vars_mean_list.model_name.iat[index] = 'na_3'\n",
    "    if standard_name=='number_of_liquid_cloud_droplets_in_cloud': \n",
    "        vars_mean_list.model_name.iat[index] = 'nlcic'\n",
    "    if do_ice:\n",
    "        if standard_name=='number_of_ice_crystals_in_cloud': \n",
    "            vars_mean_list.model_name.iat[index] = 'niic'\n",
    "    if standard_name=='dissipation_rate_of_turbulent_kinetic_energy': \n",
    "        vars_mean_list.model_name.iat[index] = 'eps'\n",
    "    if standard_name=='zonal_momentum_flux': \n",
    "        vars_mean_list.model_name.iat[index] = 'uw_zt'\n",
    "    if standard_name=='meridional_momentum_flux': \n",
    "        vars_mean_list.model_name.iat[index] = 'vw_zt'\n",
    "    if standard_name=='variance_of_upward_air_velocity': \n",
    "        vars_mean_list.model_name.iat[index] = 'w2_zt'\n",
    "    if standard_name=='vertical_flux_potential_temperature': \n",
    "        vars_mean_list.model_name.iat[index] = 'wth_zt'\n",
    "    if standard_name=='vertical_flux_liquid_ice_water_potential_temperature': \n",
    "        vars_mean_list.model_name.iat[index] = 'wthli_zt'\n",
    "    if standard_name=='vertical_flux_water_vapor': \n",
    "        vars_mean_list.model_name.iat[index] = 'wqv_zt'\n",
    "    if standard_name=='vertical_flux_total_water': \n",
    "        vars_mean_list.model_name.iat[index] = 'wqt_zt'\n",
    "    if standard_name=='area_fraction_of_liquid_cloud': \n",
    "        vars_mean_list.model_name.iat[index] = 'cloud_f'\n",
    "    if standard_name=='precipitation_flux_in_air': \n",
    "        vars_mean_list.model_name.iat[index] = 'PF'\n",
    "        vars_mean_list.conv_factor.iat[index] = 1/(3600.)\n",
    "    if standard_name=='precipitation_flux_in_air_in_ice_phase': \n",
    "        vars_mean_list.model_name.iat[index] = 'PFi'\n",
    "        vars_mean_list.conv_factor.iat[index] = 1/(3600.)\n",
    "    if standard_name=='downwelling_longwave_flux_in_air': \n",
    "        vars_mean_list.model_name.iat[index] = 'LWdn'\n",
    "    if standard_name=='upwelling_longwave_flux_in_air': \n",
    "        vars_mean_list.model_name.iat[index] = 'LWup'\n",
    "    if standard_name=='tendency_of_air_potential_temperature_due_to_radiative_heating': \n",
    "        vars_mean_list.model_name.iat[index] = 'Srad'\n",
    "        vars_mean_list.conv_factor.iat[index] = 1/3600.\n",
    "    if standard_name=='tendency_of_air_potential_temperature_due_to_microphysics': \n",
    "        vars_mean_list.model_name.iat[index] = 'dth_micro'\n",
    "        vars_mean_list.conv_factor.iat[index] = 1/3600.\n",
    "    if standard_name=='tendency_of_air_potential_temperature_due_to_mixing': \n",
    "        vars_mean_list.model_name.iat[index] = 'dth_turb'\n",
    "    if standard_name=='tendency_of_water_vapor_mixing_ratio_due_to_microphysics': \n",
    "        vars_mean_list.model_name.iat[index] = 'dq_micro'\n",
    "        vars_mean_list.conv_factor.iat[index] = 1/3.6e6\n",
    "    if standard_name=='tendency_of_water_vapor_mixing_ratio_due_to_mixing': \n",
    "        vars_mean_list.model_name.iat[index] = 'dq_turb'\n",
    "    if do_progNa:\n",
    "        if standard_name=='tendency_of_aerosol_number_due_to_warm_microphysics': \n",
    "            vars_mean_list.model_name.iat[index] = 'na_loss_liq'\n",
    "            vars_mean_list.conv_factor.iat[index] = -1.\n",
    "        if standard_name=='tendency_of_aerosol_number_due_to_mixing': \n",
    "            vars_mean_list.model_name.iat[index] = 'dna_mixing'\n",
    "        if do_ice:\n",
    "            if standard_name=='tendency_of_aerosol_number_due_to_cold_microphysics': \n",
    "                vars_mean_list.model_name.iat[index] = 'na_loss_ice'\n",
    "                vars_mean_list.conv_factor.iat[index] = -1.\n",
    "    if do_ice:\n",
    "        if standard_name=='tendency_of_ice_number_due_to_heterogeneous_freezing': \n",
    "            vars_mean_list.model_name.iat[index] = 'Sice_het'\n",
    "        if standard_name=='tendency_of_ice_number_due_to_secondary_ice_production': \n",
    "            vars_mean_list.model_name.iat[index] = 'Sice_sec'\n",
    "        if standard_name=='tendency_of_ice_number_due_to_homogeneous_freezing': \n",
    "            vars_mean_list.model_name.iat[index] = 'Sice_hom'\n",
    "\n",
    "vars_mean_list[3:] # echo variables (first rows are dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DEPHY output file\n",
    "\n",
    "Write a single file to contain all domain-mean scalar and profile outputs. This code expects the write directory to be pre-existing (already created by the user). In the case that this output will be committed to the comble-mip GitHub repository, see above \"Specify directory locations\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create DEPHY output file\n",
    "dephy_filename = './' + my_gitdir + my_outfile\n",
    "if os.path.exists(dephy_filename):\n",
    "    os.remove(dephy_filename)\n",
    "    print('The file ' + dephy_filename + ' has been deleted successfully')    \n",
    "dephy_file = Dataset(dephy_filename,mode='w',format='NETCDF3_CLASSIC')\n",
    "\n",
    "# create global attributes\n",
    "dephy_file.title='DHARMA LES results for COMBLE-MIP case: fixed Nd and Ni'\n",
    "dephy_file.reference='https://github.com/ARM-Development/comble-mip'\n",
    "dephy_file.authors='Ann Fridlind (ann.fridlind@nasa.gov) and Florian Tornow (florian.tornow@nasa.gov)'\n",
    "dephy_file.source=input_filename\n",
    "dephy_file.version=dt.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "dephy_file.format_version='DEPHY SCM format version 1.6'\n",
    "dephy_file.script='convert_DHARMA_LES_output_to_dephy_format.ipynb'\n",
    "dephy_file.startDate=start_dtime\n",
    "dephy_file.force_geo=1\n",
    "dephy_file.surfaceType='ocean (after spin-up)'\n",
    "dephy_file.surfaceForcing='ts (after spin-up)'\n",
    "dephy_file.lat=str(dharma_params['Coriolis'].lat) + ' deg N'\n",
    "dephy_file.dx=str(dharma_params['geometry'].L_x/dharma_params['geometry'].nx) + ' m'\n",
    "dephy_file.dy=str(dharma_params['geometry'].L_y/dharma_params['geometry'].ny) + ' m'\n",
    "dephy_file.dz='see zf variable'\n",
    "dephy_file.nx=dharma_params['geometry'].nx\n",
    "dephy_file.ny=dharma_params['geometry'].ny\n",
    "dephy_file.nz=dharma_params['geometry'].nz\n",
    "\n",
    "# create dimensions\n",
    "nz = dharma_snds.sizes['zt']\n",
    "zf = dephy_file.createDimension('zf', nz)\n",
    "zf = dephy_file.createVariable('zf', np.float64, ('zf',))\n",
    "zf.units = 'm'\n",
    "zf.long_name = 'height'\n",
    "zf[:] = dharma_snds['zt'].data\n",
    "\n",
    "ze = dephy_file.createDimension('ze', nz)\n",
    "ze = dephy_file.createVariable('ze', np.float64, ('ze',))\n",
    "ze.units = 'm'\n",
    "ze.long_name = 'layer_top_height'\n",
    "ze[:] = dharma_snds['zw'].data[1:]\n",
    "\n",
    "nt = dharma_snds.sizes['time']\n",
    "time = dephy_file.createDimension('time', nt)\n",
    "time = dephy_file.createVariable('time', np.float64, ('time',))\n",
    "time.units = 'seconds since ' + dephy_file.startDate\n",
    "time.long_name = 'time'\n",
    "# find time step and build time in seconds\n",
    "delta_t = (dharma_snds['time'].data[1]-dharma_snds['time'].data[0])/np.timedelta64(1, \"s\")\n",
    "time[:] = np.arange(nt)*delta_t\n",
    "\n",
    "# create and fill variables\n",
    "for index in vars_mean_list.index[2:]:\n",
    "    std_name = vars_mean_list.standard_name.iat[index]\n",
    "#   print(std_name) # debug\n",
    "    var_name = vars_mean_list.variable_id.iat[index]\n",
    "    mod_name = vars_mean_list.model_name.iat[index]\n",
    "    c_factor = vars_mean_list.conv_factor.iat[index]\n",
    "    if vars_mean_list.dimensions.iat[index]=='time':\n",
    "        new_sca = dephy_file.createVariable(var_name, np.float64, ('time'))\n",
    "        new_sca.units = vars_mean_list.units.iat[index]\n",
    "        new_sca.long_name = std_name\n",
    "        if vars_mean_list.model_name.iat[index]!='missing data' and mod_name in dharma_scas:\n",
    "            if (var_name == 'z0'):\n",
    "                new_sca[:] = np.round(dharma_scas[mod_name].data*c_factor,4)\n",
    "            elif (var_name == 'z0h')| (var_name == 'z0q'):\n",
    "                new_sca[:] = np.round(dharma_scas[mod_name].data*c_factor,7)\n",
    "            else:\n",
    "                new_sca[:] = dharma_scas[mod_name].data*c_factor\n",
    "    if vars_mean_list.dimensions.iat[index]=='time, height':\n",
    "        new_snd = dephy_file.createVariable(var_name, np.float64, ('time','zf'))\n",
    "        new_snd.units = vars_mean_list.units.iat[index]\n",
    "        new_snd.long_name = std_name\n",
    "        if vars_mean_list.model_name.iat[index]!='missing data' and mod_name in dharma_snds: \n",
    "            new_snd[:] = dharma_snds[mod_name].data*c_factor\n",
    "\n",
    "print(dephy_file)\n",
    "dephy_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dephy_check = xr.open_dataset(dephy_filename)\n",
    "dephy_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dephy_check['z0h'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
